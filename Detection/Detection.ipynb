{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cea83672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA`_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5909d1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee36b76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf7ed9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '/home/gene/Documents/Senti/Fathom/AI-Detector/ExampleData/'\n",
    "dataset = 'PubMed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1aaad2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "path = os.path.join(root_path,dataset)\n",
    "with open(os.path.join(path,'exp-data-AID-feature.json')) as f:\n",
    "    Features = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f85a9d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0b8a8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "Keys = ['abstract','gen_abstract','pol_abstract','mix_abstract']\n",
    "feature_names = ['GPTConf','T5Conf','entropy',]\n",
    "raw_x = np.zeros((len(Features['entropy'])*4,9*3))\n",
    "raw_y = np.zeros((len(Features['entropy'])*4,))\n",
    "index = np.zeros((len(Features['entropy'])*4,))\n",
    "                 \n",
    "for i in range(len(Features['entropy'])):\n",
    "    if i ==363:\n",
    "        continue\n",
    "    for j in range(len(Keys)):\n",
    "        raw_y[4*i+j] = min(j,1)\n",
    "        index[4*i+j] = j\n",
    "        for k in range(len(feature_names)):\n",
    "            raw_x[4*i+j,9*k] = Features[feature_names[k]][i][Keys[j]]\n",
    "            raw_x[4*i+j,9*k+1:9*k+9] = np.array(Features[feature_names[k]][i]['sum_'+Keys[j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df3826a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcd7673e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b2544ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(y,haty):\n",
    "    s = precision_score(y,haty),recall_score(y,haty),f1_score(y,haty), f1_score(y,haty,average='micro'),f1_score(y,haty,average='macro'),roc_auc_score(y,haty)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1f13c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "def evaluation(model,test_images,test_labels):\n",
    "    model = model.eval()\n",
    "    bz = 32\n",
    "    NUM = int(np.ceil(len(test_images)/bz))\n",
    "    ys = []\n",
    "    for i in range(NUM):\n",
    "        s,e = i*bz,(i+1)*bz\n",
    "        if e>len(test_images):\n",
    "            e = len(test_images)\n",
    "        x = test_images[s:e]\n",
    "        x = torch.FloatTensor(x).cuda()\n",
    "        y_hat = model(x)\n",
    "        y_hat = y_hat.detach().to('cpu').numpy()\n",
    "        ys.append(y_hat)\n",
    "    ys = np.concatenate(ys,axis=0)\n",
    "    logit = ys.argmax(axis=-1)\n",
    "    label = test_labels\n",
    "\n",
    "    \n",
    "    return metric(label,logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "065b823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):   \n",
    "    def __init__(self,feature_num):\n",
    "        super(Model, self).__init__()        \n",
    "        self.linear_layers = nn.Sequential(\n",
    "            #nn.LazyLinear(512),\n",
    "           nn.Linear(feature_num, feature_num), #41472 25088\n",
    "           nn.ReLU(inplace=True),\n",
    "            nn.Linear(feature_num, feature_num),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(feature_num, feature_num),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(feature_num, feature_num),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(feature_num, 2),\n",
    "        )\n",
    "    \n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x,y=None):\n",
    "        hat_y = self.linear_layers(x)\n",
    "        if not y ==None:\n",
    "            loss = self.loss_fn(hat_y, y)\n",
    "            return loss,hat_y\n",
    "        else:\n",
    "            return hat_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293a18c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2af216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(m,sum_num = 8,used_feature = ['Conf','entropy','lengths'],):\n",
    "    feature_names = ['gptb_Conf','t5l_Conf','entropy']\n",
    "    sum_feature = (1+sum_num)*len(used_feature)\n",
    "\n",
    "    valid_features = []\n",
    "    for k in range(len(feature_names)):\n",
    "        if feature_names[k] in used_feature:\n",
    "            valid_features.append(9*k)\n",
    "            valid_features += [9*k+1+j for j in range(sum_num)]\n",
    "    valid_features = np.array(valid_features)\n",
    "\n",
    "    x = raw_x[index<m]\n",
    "    y = raw_y[index<m]\n",
    "    x = np.transpose(x,(1,0))\n",
    "    x = x[valid_features]\n",
    "    x = np.transpose(x,(1,0))\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dea2f85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(x,y,i=0,k=2,n=5):\n",
    "    num = len(x)//k\n",
    "    bias = len(x)//(k*n)\n",
    "    assert i>=0 and i<n\n",
    "    s = bias*i\n",
    "    e = bias*i + num\n",
    "    train_x = x[s:e]\n",
    "    train_y = y[s:e]\n",
    "    test_x = np.concatenate([x[:s],x[e:]])\n",
    "    test_y = np.concatenate([y[:s],y[e:]])\n",
    "    return train_x,train_y,test_x,test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f5c620",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6b73c00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(m,sum_num,used_feature,k = 2,n = 5):\n",
    "    x,y = make_data(m,sum_num,used_feature)\n",
    "    scores = []\n",
    "    bz = 16\n",
    "\n",
    "    for i in range(n):\n",
    "        train_x,train_y,test_x,test_y = split_data(x,y,i,k,n)\n",
    "        model = Model(x.shape[1])\n",
    "        model = model.cuda()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "        for t in range(150):\n",
    "            for j in range(int(np.ceil(len(train_x)/bz))):\n",
    "                s = j*bz\n",
    "                ed = s+bz\n",
    "                ed = min(ed,len(train_x))\n",
    "                x0 = train_x[s:ed]\n",
    "                y0 = train_y[s:ed]\n",
    "                x0 = torch.FloatTensor(x0).cuda()\n",
    "                y0 = torch.LongTensor(y0).cuda()\n",
    "                loss, y_hat = model(x0,y0)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        s = evaluation(model,test_x,test_y)\n",
    "        scores.append(s)\n",
    "        print(s)\n",
    "        \n",
    "    scores = np.array(scores)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "289593f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8333333333333334, 0.9388646288209607, 0.8829568788501027, 0.8131147540983606, 0.7097711223518806, 0.6865375775683751)\n",
      "(0.7902097902097902, 0.9890590809628009, 0.8785228377065113, 0.7950819672131149, 0.6120362879632033, 0.6023726777363024)\n",
      "(0.7854671280276817, 0.9912663755458515, 0.8764478764478765, 0.7901639344262295, 0.5903978512674165, 0.5877384509308204)\n",
      "(0.7491803278688525, 1.0, 0.8566073102155577, 0.7491803278688525, 0.42830365510777885, 0.5)\n",
      "(0.7508196721311475, 1.0, 0.8576779026217228, 0.7508196721311475, 0.4288389513108614, 0.5)\n"
     ]
    }
   ],
   "source": [
    "K = 8\n",
    "Setting = 'Setting-3'\n",
    "used_feature = ['T5Conf','GPTConf','entropy']\n",
    "\n",
    "print('precision, recall,f1, micro_f1, macro_f1, auc')\n",
    "s = {'Setting-1':2,'Setting-2':3,'Setting-3':4}[Setting]\n",
    "scores = train(s,K,used_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df07ffaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b34f94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b14d314",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
